@inproceedings{Lawall2018,
   abstract = {The Coccinelle C-program matching and transformation tool was first released in 2008 to facilitate specification and automation in the evolution of Linux kernel code. The novel contribution of Coccinelle was that it allows software developers to write code manipulation rules in terms of the code structure itself, via a generalization of the patch syntax. Over the years, Coccinelle has been extensively used in Linux kernel development, resulting in over 6000 commits to the Linux kernel, and has found its place as part of the Linux kernel development process. This paper studies the impact of Coccinelle on Linux kernel development and the features of Coccinelle that have made it possible. It can provide guidance on how other research-based tools can achieve practical impact in the open-source development community.},
   author = {Julia Lawall and Gilles Muller},
   city = {Boston, MA},
   isbn = {978-1-939133-01-4},
   journal = {2018 USENIX Annual Technical Conference (USENIX ATC 18)},
   month = {7},
   pages = {601-614},
   publisher = {USENIX Association},
   title = {Coccinelle: 10 Years of Automated Evolution in the Linux Kernel},
   url = {https://www.usenix.org/conference/atc18/presentation/lawall},
   year = {2018},
}
@mastersthesis{Gomes2021,
   author = {Nuno Miguel Rodrigues Gomes},
   city = {Porto},
   institution = {Universidade do Porto},
   keywords = {Ciências da engenharia e tecnologias::Engenharia electrotécnica,Electrical engineering,Electronic engineering,Engenharia electrotécnica,Engineering and technology::Electrical engineering,Information engineering,electrónica e informática},
   month = {5},
   title = {An Analysis of Performance Recipes with C Applications},
   url = {https://repositorio-aberto.up.pt/handle/10216/133679},
   year = {2021},
}
@article{Arabnejad2020,
   abstract = {Directive-driven programming models, such as OpenMP, are one solution for exploring the potential parallelism when targeting multicore architectures. Although these approaches significantly help developers, code parallelization is still a non-trivial and time-consuming process, requiring parallel programming skills. Thus, many efforts have been made toward automatic parallelization of the existing sequential code. This article presents AutoPar-Clava, an OpenMP-based automatic parallelization compiler which: (1) statically detects parallelizable loops in C applications; (2) classifies variables used inside the target loop based on their access pattern; (3) supports reduction clauses on scalar and array variables whenever it is applicable; and (4) generates a C OpenMP parallel code from the input sequential version. The effectiveness of AutoPar-Clava is evaluated by using the NAS and Polyhedral Benchmark suites and targeting a x86-based computing platform. The achieved results are very promising and compare favorably with closely related auto-parallelization compilers, such as Intel C/C++ Compiler (icc), ROSE, TRACO and CETUS.},
   author = {Hamid Arabnejad and João Bispo and João M P Cardoso and Jorge G Barbosa},
   doi = {10.1007/s11227-019-03109-9},
   isbn = {0123456789},
   journal = {The Journal of Supercomputing},
   keywords = {Code transformations,Compiler framework,Parallel programming,Static code analysis},
   pages = {6753-6785},
   title = {Source-to-source compilation targeting OpenMP-based automatic parallelization of C applications},
   volume = {76},
   url = {https://doi.org/10.1007/s11227-019-03109-9},
   year = {2020},
}
@mastersthesis{,
   author = {Sunna Berglind Sigurðardóttir},
   city = {Reykjavík},
   institution = {University of Iceland},
   keywords = {Hugbúnaðargerð,Hugbúnaðarverkfræði,Thesis},
   month = {5},
   title = {A refactoring catalogue and tool for refactoring C/C++ HPC code},
   url = {https://skemman.is/handle/1946/35941 https://skemman.is/bitstream/1946/35941/1/HPC_refactoring_catalogue_and_tool.pdf},
   year = {2020},
}
@article{Susungi2021,
   abstract = {While compilers generally support parallel programming languages and APIs, their internal program representations are mostly designed from the sequential programs standpoint (exceptions include source-to-source parallel compilers, for instance). This makes the integration of compilation techniques dedicated to parallel programs more challenging. In addition, parallelism has various levels and different targets, each of them with specific characteristics and constraints. With the advent of multi-core processors and general purpose accelerators , parallel computing is now a common and pervasive consideration. Thus, software support to parallel programming activities is essential to make this technical transition more realistic and beneficial. The case of compilers is fundamental as they deal with (parallel) programs at a structural level, thus the need for intermediate representations. This article surveys and discusses attempts to provide intermediate representations for the proper support of explicitly parallel programs. We highlight the gap between available contributions and their concrete implementation in compilers and then exhibit possible future research directions.},
   author = {Adilla Susungi and Claude Tadonki},
   doi = {10.1145/3452299},
   journal = {Comput. Surv},
   keywords = {CCS Concepts: • Computing methodologies Additional Key Words and Phrases: Intermediate representation,compilation,graph ACM Reference format: Adilla Susungi and Claude Tadonki 2020 Intermediate Representations for Explicitly Parallel Programs ACM,intermediate language,parallelism,polyhedral model},
   title = {00 Intermediate Representations for Explicitly Parallel Programs},
   volume = {54},
   url = {https://doi.org/10.1145/3452299},
   year = {2021},
}
@article{Chen1993,
   abstract = {This paper shows that code expanding optimizations have strong and nonintuitive implications on instruction cache design. Three types of code expanding optimizations are studied in this paper: instruction placement, function inline expansion, and superscalar optimizations. Overall, instruction placement reduces the miss ratio of small caches. Function inline expansion improves the performance for small cache sizes, but degrades the performance of medium caches. Superscalar optimizations increase the miss ratio for all cache sizes. However, they also increase the sequentiality of instruction access so that a simple load forwarding scheme effectively cancels the negative effects. Overall, we show that with load forwarding, the three types of code expanding optimizations jointly improve the performance of small caches and have little effect on large caches. © 1993 IEEE},
   author = {William Y. Chen and Thomas M. Conte and Wen Mei W. Hwu},
   doi = {10.1109/12.241594},
   issn = {00189340},
   issue = {9},
   journal = {IEEE Transactions on Computers},
   keywords = {C compiler,cache memory,code expansion,code optimization,function inline expansion,instruction placement,load forwarding,superscalar optimizations},
   pages = {1045-1057},
   title = {The Effect of Code Expanding Optimizations on Instruction Cache Design},
   volume = {42},
   year = {1993},
}
@misc{RustMirRfc,
   author = {Niko Matsakis},
   title = {Rust Language RFC 1211 - MIR},
   howpublished = {Available at: \url{https://github.com/rust-lang/rfcs/pull/1211}},
   note = {Accessed 2022-02-02},
}
@misc{LLVMCodegen,
   title = {The LLVM Target-Independent Code Generator — LLVM 15.0.0git documentation},
   howpublished = {Available at: \url{https://llvm.org/docs/CodeGenerator.html}},
   note = {Accessed 2022-02-02},
}
@misc{LLVMDirectExecution,
   title = {lli - directly execute programs from LLVM bitcode — LLVM 15.0.0git documentation},
   howpublished = {Available at: \url{https://llvm.org/docs/CommandGuide/lli.html}},
   note = {Accessed 2022-02-02},
}
@misc{LattnerAOSA,
   author = {Chris Lattner},
   title = {The Architecture of Open Source Applications: LLVM},
   howpublished = {Available at: \url{http://www.aosabook.org/en/llvm.html}},
   note = {Accessed 2022-02-02},
}
@misc{CirRfc,
   author = {Bernardo Cardoso Lopes and Nathan Lanza},
   title = {[RFC] An MLIR based Clang IR (CIR) - Clang Frontend - LLVM Discussion Forums},
   howpublished = { 2022. Available at \url{https://discourse.llvm.org/t/rfc-an-mlir-based-clang-ir-cir/63319}},
   note = {Accessed 2022-07-05},
   year = {2022},
}
@article{Necula2002,
   abstract = {This paper describes the C Intermediate Language: a highlevel representation along with a set of tools that permit easy analysisand source-to-source transformation of C programs. Compared to C, CIL has fewer constructs. It breaks down certain complicated constructs of C into simpler ones, and thus it works at a lower level than abstract-syntax trees. But CIL is also more high-level than typical intermediate languages (e.g., three-address code) designed for compilation. As a result, what we have is a representation that makes it easy to analyze and manipulate C programs, and emit them in a form that resembles the original source. Moreover, it comes with a front-end that translates to CIL not only ANSI C programs but also those using Microsoft C or GNU C extensions. We describe the structure of CIL with a focus on how it disambiguates those features of C that we found to be most confusing for program analysis and transformation. We also describe a whole-program merger based on structural type equality, allowing a complete project to be viewed as a single compilation unit. As a representative application of CIL, we show a transformation aimed at making code immune to stack-smashing attacks. We are currently using CIL as part of a system that analyzes and instruments C programs with run-time checks to ensure type safety. CIL has served us very well in this project, and we believe it can usefully be applied in other situations as well.},
   author = {George C. Necula and Scott McPeak and Shree P. Rahul and Westley Weimer},
   doi = {10.1007/3-540-45937-5_16/COVER/},
   isbn = {3540433694},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {213-228},
   publisher = {Springer Verlag},
   title = {CIL: Intermediate language and tools for analysis and transformation of C programs},
   volume = {2304},
   url = {https://link.springer.com/chapter/10.1007/3-540-45937-5_16},
   year = {2002},
}
@misc{Duff1988,
   author = {Tom Duff},
   title = {"Tom Duff on Duff's Device"},
   howpublished = {Email correspondence, archived at: \url{http://www.lysator.liu.se/c/duffs-device.html}},
   note = {Accessed 2022-07-01},
   year = {1988},
}
@book{Aho2006,
   author = {Alfred V Aho and Monica S Lam and Ravi Sethi and Jeffrey D Ullman},
   city = {USA},
   isbn = {0321486811},
   publisher = {Addison-Wesley Longman Publishing Co., Inc.},
   title = {Compilers: Principles, Techniques, and Tools (2nd Edition)},
   year = {2006},
}
@article{Tristan2009,
   abstract = {Translation validation establishes a posteriori the correctness of a run of a compilation pass or other program transformation. In this paper, we develop an efficient translation validation algorithm for the Lazy Code Motion (LCM) optimization. LCM is an interesting challenge for validation because it is a global optimization that moves code across loops. Consequently, care must be taken not to move computations that may fail before loops that may not terminate. Our validator includes a specific check for anticipability to rule out such incorrect moves. We present a mechanically-checked proof of correctness of the validation algorithm, using the Coq proof assistant. Combining our validator with an unverified implementation of LCM, we obtain a LCM pass that is provably semantics-preserving and was integrated in the CompCert formally verified compiler. Copyright © 2009 ACM.},
   author = {Jean Baptiste Tristan and Xavier Leroy},
   doi = {10.1145/1543135.1542512},
   issn = {15232867},
   issue = {6},
   journal = {ACM SIGPLAN Notices},
   keywords = {Lazy code motion,Redundancy elimination,The Coq proof assistant,Translation validation,Verified compilers},
   pages = {316-326},
   publisher = {Association for Computing Machinery},
   title = {Verified validation of lazy code motion},
   volume = {44},
   year = {2009},
}
@article{Leroy2009Backend,
   abstract = {This article describes the development and formal verification (proof of semantic preservation) of a compiler back-end from Cminor (a simple imperative intermediate language) to PowerPC assembly code, using the Coq proof assistant both for programming the compiler and for proving its soundness. Such a verified compiler is useful in the context of formal methods applied to the certification of critical software: the verification of the compiler guarantees that the safety properties proved on the source code hold for the executable compiled code as well. © 2009 Springer Science+Business Media B.V.},
   author = {Xavier Leroy},
   doi = {10.1007/S10817-009-9155-4},
   issn = {01687433},
   issue = {4},
   journal = {Journal of Automated Reasoning},
   keywords = {Compiler transformations and optimizations,Compiler verification,Formal methods,Program proof,Semantic preservation,The Coq theorem prover},
   month = {1},
   pages = {363-446},
   title = {A formally verified compiler back-end},
   volume = {43},
   year = {2009},
}
@article{Kildall1973,
   abstract = {A technique is presented for global analysis of program structure in order to perform compile time optimization of object code generated for expressions. The global expression optimization presented includes constant propagation, common subexpression elimination, elimination of redundant register load operations, and live expression analysis. A general purpose program flow analysis algorithm is developed which depends upon the existence of an "optimizing function." The algorithm is defined formally using a directed graph model of program flow structure, and is shown to be correct. Several optimizing functions are defined which, when used in conjunction with the flow analysis algorithm, provide the various forms of code optimization. The flow analysis algorithm is sufficiently general that additional functions can easily be defined for other forms of global code optimization.},
   author = {Gary A. Kildall},
   doi = {10.1145/512927.512945},
   issn = {07308566},
   journal = {Conference Record of the Annual ACM Symposium on Principles of Programming Languages},
   month = {1},
   pages = {194-206},
   publisher = {Association for Computing Machinery},
   title = {A unified approach to global program optimization},
   year = {1973},
}
@article{Blazy2005,
   abstract = {This paper presents a formal verification with the Coq proof assistant of a memory model for C-like imperative languages. This model defines the memory layout and the operations that manage the memory. The model has been specified at two levels of abstraction and implemented as part of an ongoing certification in Coq of a moderatelyoptimising C compiler. Many properties of the memory have been verified in the specification. They facilitate the definition of precise formal semantics of C pointers. A certified O Caml code implementing the memory model has been automatically extracted from the specifications. © Springer-Verlag Berlin Heidelberg 2005.},
   author = {Sandrine Blazy and Xavier Leroy},
   doi = {10.1007/11576280_20},
   isbn = {3540297979},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {280-299},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Formal Verification of a Memory Model for C-Like Imperative Languages},
   volume = {3785 LNCS},
   url = {https://link.springer.com/chapter/10.1007/11576280_20},
   year = {2005},
}
@article{Barthe2002,
   abstract = {Bytecode verification is one of the key security functions of the JavaCard architecture. Its correctness is often cast relatively to a defensive virtual machine that performs checks at run-time, and an offensive one that does not, and can be summarized as stating that the two machines coincide on programs that pass bytecode verification. We review the process of establishing such a correctness statement in a proof assistant, and focus in particular on the problem of automating the construction of an offensive virtual machine and a bytecode verifier from a defensive machine. © 2002, Springer-Verlag Berlin Heidelberg.},
   author = {Gilles Barthe and Pierre Courtieu and Guillaume Dufay and Simão Melo De Sousa},
   doi = {10.1007/3-540-45719-4_4},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {41-59},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Tool-Assisted Specification and Verification of the JavaCard Platform},
   volume = {2422},
   url = {https://link.springer.com/chapter/10.1007/3-540-45719-4_4},
   year = {2002},
}
@article{Barthe2014,
   abstract = {CompCert is a formally verified compiler that generates compact and efficient code for a large subset of the C language. However, CompCert foregoes using SSA, an intermediate representation employe...},
   author = {Gilles Barthe and Delphine Demange and David Pichardie},
   doi = {10.1145/2579080},
   issn = {15584593},
   issue = {1},
   journal = {ACM Transactions on Programming Languages and Systems (TOPLAS)},
   keywords = {Demange,G,Reliability,Single static assignment,Verification Additional Key Words and Phrases: Single static assignment,and Pichardie,compiler verification,mechanized proof,mechanized proof ACM Reference Format: Barthe},
   month = {3},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {Formal Verification of an SSA-Based Middle-End for CompCert},
   volume = {36},
   url = {https://dl.acm.org/doi/abs/10.1145/2579080},
   year = {2014},
}
@article{Leroy2009Compiler,
   abstract = {This paper reports on the development and formal verification (proof of semantic preservation) of CompCert, a compiler from Clight (a large subset of the C programming language) to PowerPC assembly code, using the Coq proof assistant both for programming the compiler and for proving its correctness. Such a verified compiler is useful in the context of critical software and its formal verification: the verification of the compiler guarantees that the safety properties proved on the source code hold for the executable compiled code as well.},
   author = {Xavier Leroy},
   doi = {10.1145/1538788.1538814},
   issue = {7},
   pages = {107},
   title = {Formal Verification of a Realistic Compiler},
   volume = {52},
   year = {2009},
}
@article{Blazy2009,
   abstract = {This article presents the formal semantics of a large subset of the C language called Clight. Clight includes pointer arithmetic, struct and union types, C loops and structured switch statements. Clight is the source language of the CompCert verified compiler. The formal semantics of Clight is a big-step operational semantics that observes both terminating and diverging executions and produces traces of input/output events. The formal semantics of Clight is mechanized using the Coq proof assistant. In addition to the semantics of Clight, this article describes its integration in the CompCert verified compiler and several ways by which the semantics was validated.},
   author = {Sandrine Blazy and Xavier Leroy},
   doi = {10.1007/s10817-009-9148-3},
   journal = {J Autom Reasoning},
   keywords = {Coq proof assistant,Formal proof ·,Mechanized semantics ·,Operational semantics ·,The,The C programming language ·},
   pages = {263-288},
   title = {Mechanized Semantics for the Clight Subset of the C Language},
   volume = {43},
   url = {http://compcert.inria.fr.},
   year = {2009},
}
@article{Demange2015,
   abstract = {The Static Single Assignment (SSA) form is a predominant technology in modern compilers, enabling powerful and fast program optimizations. Despite its great success in the implementation of production compilers, it is only very recently that this technique has been introduced in verified compilers. As of today, few evidence exist on that, in this context, it also allows faster and simpler optimizations. This work builds on the CompCertSSA verified compiler (an SSA branch of the verified CompCert C compiler). We implement and verify two prevailing SSA optimizations: Sparse Conditional Constant Propagation and Global Value Numbering. For both transformations, we mechanically prove their soundness in the Coq proof assistant. Both optimization proofs are embedded in a single sparse optimization framework, factoring out many of the dominance-based reasoning steps required in proofs of SSA-based optimizations. Our experimental evaluations indicate both a better precision, and a significant compilation time speedup.},
   author = {Delphine Demange and David Pichardie and Léo Stefanesco},
   doi = {10.1007/978-3-662-46663-6_12},
   title = {LNCS 9031 - Verifying Fast and Sparse SSA-Based Optimizations in Coq},
   url = {http://www.irisa.fr/celtique/ext/ssa},
   year = {2015},
}
@article{Logozzo2008,
   abstract = {We discuss the challenges faced by bytecode analyzers designed for code verification compared to similar analyzers for source code. While a bytecode-level analysis brings many simplifications, e.g., fewer cases, independence from source syntax, name resolution, etc., it also introduces precision loss that must be recovered either via preprocess-ing, more precise abstract domains, more precise transfer functions, or a combination thereof. The paper studies the relative completeness of a static analysis for byte-code compared to the analysis of the program source. We illustrate it through examples originating from the design and the implementation of Clousot, a generic static analyzer based on Abstract Interpretation for the analysis of MSIL.},
   author = {Francesco Logozzo and Manuel Fähndrich},
   doi = {10.1007/978-3-540-78791-4_14},
   title = {On the Relative Completeness of Bytecode Analysis versus Source Code Analysis},
   year = {2008},
}
@article{Zangerl2018,
   abstract = {Optimizing compilers provide valuable contributions to the quality of processed code. The vast majority of application developers rely on those capabilities to obtain binary code efficiently utilizing processing resources. However, compiler proficiency is frequently misjudged by application developers. While for some constellations the effectiveness of those optimizations is grossly underestimated, for others, mostly involving higher-level semantic concepts of embedded DSLs, the compilers' influence on the code quality tends to disappoint. In this paper, we provide examples for the effectiveness and ineffectiveness of state-of-the-art optimizing compilers in improving application code. Based on those observations we characterize the differences between positive and negative examples and provide an in-depth explanation for the short-comings of optimizing compilers. Furthermore, we present a semantic-aware compiler architecture rectifying those deficiencies and outline several example use cases demonstrating our architecture's ability to contribute to the efficiency and the capabilities of embedded DSLs.},
   author = {Peter Zangerl and Herbert Jordan and Peter Thoman and Philipp Gschwandtner and Thomas Fahringer},
   doi = {10.1145/3229631.3239371},
   isbn = {9781450364942},
   journal = {ACM International Conference Proceeding Series},
   month = {7},
   pages = {195-201},
   publisher = {Association for Computing Machinery},
   title = {Exploring the semantic gap in compiling embedded DSLs},
   year = {2018},
}
@article{Click1995,
   abstract = {Modern optimizing compilers use several passes over a program's intermediate representation to generate good code. Many of these optimization exhibit a phase-ordering problem. Getting the best code may require iterating optimizations until a fixed point is reached. Combining these phases can lead to the discovery of more facts about the program, exposing more opportunities for optimization. This article presents a framework for describing optimizations. It shows how to combine two such frameworks and how to reason about the properties of the resulting framework. The structure of the framework provides insight into when a combination yields better results. To make the ideas more concrete, this article presents a framework for combining constant propagation, value numbering, and unreachable-code elimination. It is an open question as to what other frameworks can be combined in this way.},
   author = {Cliff Click and Keith D Cooper},
   doi = {10.1145/201059.201061},
   keywords = {D34 [Programming Languages]: Processors-compilers; optimization General Terms: Algorithms,Theory Additional Key Words and Phrases: Constant propagation,data-flow analysis,optimizing compil-ers,value numbering},
   title = {Combining Analyses, Combining Optimization},
   year = {1995},
}
@article{Lattner2004,
   abstract = {This paper describes LLVM (Low Level Virtual Machine), a compiler framework designed to support transparent, lifelong program analysis and transformation for arbitrary programs , by providing high-level information to compiler transformations at compile-time, link-time, run-time, and in idle time between runs. LLVM defines a common, low-level code representation in Static Single Assignment (SSA) form, with several novel features: a simple, language-independent type-system that exposes the primitives commonly used to implement high-level language features; an instruction for typed address arithmetic; and a simple mechanism that can be used to implement the exception handling features of high-level languages (and setjmp/longjmp in C) uniformly and efficiently. The LLVM compiler framework and code representation together provide a combination of key capabilities that are important for practical, lifelong analysis and transformation of programs. To our knowledge, no existing compilation approach provides all these capabilities. We describe the design of the LLVM representation and compiler framework, and evaluate the design in three ways: (a) the size and effectiveness of the representation, including the type information it provides; (b) compiler performance for several interprocedural problems; and (c) illustrative examples of the benefits LLVM provides for several challenging compiler problems.},
   author = {Chris Lattner and Vikram Adve},
   doi = {10.1109/CGO.2004.1281665},
   title = {LLVM: A Compilation Framework for Lifelong Program Analysis \& Transformation},
   url = {http://llvm.cs.uiuc.edu/},
   year = {2004},
}
@article{Cytron1991,
   abstract = {In optimizing compilers, data structure choices directly influence the power and efficiency of practical program optimization. A poor choice of data structure can inhibit optimization or slow compilation to the point that advanced optimization features become undesirable. Recently, static single assignment form and the control dependence graph have been proposed to represent data flow and control flow propertiee of programs. Each of these previously unrelated techniques lends efficiency and power to a useful class of program optimization. Although both of these structures are attractive, the difficulty of their construction and their potential size have discouraged their use. We present new algorithms that efficiently compute these data structures for arbitrary control flow graphs. The algorithms use dominance frontiers, a new concept that may have other applications. We also give analytical and experimental evidence that all of these data structures are usually linear in the size of the original program. This paper thus presents strong evidence that these structures can be of practical use in optimization.},
   author = {Ron Cytron and Jeanne Ferrante and Barry K. Rosen and Mark N. Wegman and F. Kenneth Zadeck},
   doi = {10.1145/115372.115320},
   issn = {15584593},
   issue = {4},
   journal = {ACM Transactions on Programming Languages and Systems (TOPLAS)},
   keywords = {control dependence,control flow graph,def-use chain,dominator,optimization,optimizing compilers,procedures, functions and subroutines},
   month = {10},
   pages = {451-490},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {Efficiently computing static single assignment form and the control dependence graph},
   volume = {13},
   url = {https://dl.acm.org/doi/abs/10.1145/115372.115320},
   year = {1991},
}
@article{Gysi2021,
   abstract = {Most compilers have a single core intermediate representation (IR) (e.g., LLVM) sometimes complemented with vaguely defined IR-like data structures. This IR is commonly low-level and close to machi...},
   author = {Tobias Gysi and Christoph Müller and Oleksandr Zinenko and Stephan Herhut and Eddie Davis and Tobias Wicky and Oliver Fuhrer and Torsten Hoefler and Tobias Grosser},
   doi = {10.1145/3469030},
   issn = {15443973},
   issue = {4},
   journal = {ACM Transactions on Architecture and Code Optimization (TACO)},
   keywords = {E Davis, T Wicky, and O Fuhrer, Vulcan Inc, USA,Weather and climate,emails: \{EddieD, TobiasW, OliverF\}@,intermediate representations,stencil computations,• Applied computing → Earth and atmospheric sciences},
   month = {9},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {Domain-Specific Multi-Level IR Rewriting for GPU},
   volume = {18},
   url = {https://dl.acm.org/doi/abs/10.1145/3469030},
   year = {2021},
}
@article{Lattner2021,
   abstract = {This work presents MLIR, a novel approach to building reusable and extensible compiler infrastructure. MLIR addresses software fragmentation, compilation for heterogeneous hardware, significantly reducing the cost of building domain specific compilers, and connecting existing compilers together. MLIR facilitates the design and implementation of code generators, translators and optimizers at different levels of abstraction and across application domains, hardware targets and execution environments. The contribution of this work includes (1) discussion of MLIR as a research artifact, built for extension and evolution, while identifying the challenges and opportunities posed by this novel design, semantics, optimization specification, system, and engineering. (2) evaluation of MLIR as a generalized infrastructure that reduces the cost of building compilers-describing diverse use-cases to show research and educational opportunities for future programming languages, compilers, execution environments, and computer architecture. The paper also presents the rationale for MLIR, its original design principles, structures and semantics.},
   author = {Chris Lattner and Mehdi Amini and Uday Bondhugula and Albert Cohen and Andy Davis and Jacques Pienaar and River Riddle and Tatiana Shpeisman and Nicolas Vasilache and Oleksandr Zinenko},
   doi = {10.1109/CGO51591.2021.9370308},
   isbn = {9781728186139},
   journal = {CGO 2021 - Proceedings of the 2021 IEEE/ACM International Symposium on Code Generation and Optimization},
   month = {2},
   pages = {2-14},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {MLIR: Scaling Compiler Infrastructure for Domain Specific Computation},
   year = {2021},
}
@article{Arabnejad2018,
   abstract = {Automatic parallelization of sequential code has become increasingly relevant in multicore programming. In particular, loop paral-lelization continues to be a promising optimization technique for scientiï¿¿c applications, and can provide considerable speedups for program execution. Furthermore, if we can verify that there are no true data dependencies between loop iterations, they can be easily parallelized. This paper describes Clava AutoPar, a library for the Clava weaver that performs automatic and symbolic parallelization of C code. The library is composed of two main parts, parallel loop detection and source-to-source code parallelization. The system is entirely automatic and attempts to statically detect parallel loops for a given input program, without any user intervention or proï¿¿l-ing information. We obtained a geometric mean speedup of 1.5 for a set of programs from the C version of the NAS benchmark, and experimental results suggest that the performance obtained with Clava AutoPar is comparable or better than other similar research and commercial tools.},
   author = {Hamid Arabnejad and João Bispo and Jorge G Barbosa and João M P Cardoso},
   city = {New York, New York, USA},
   doi = {10.1145/3183767},
   isbn = {9781450364447},
   journal = {Proceedings of the 9th Workshop and 7th Workshop on Parallel Programming and RunTime Management Techniques for Manycore Architectures and Design Tools and Architectures for Multicore Embedded Computing Platforms  - PARMA-DITAM '18},
   keywords = {Automatic parallelization,OpenMP,Parallel Programming,source-to-source Compilation},
   publisher = {ACM Press},
   title = {AutoPar-Clava: An Automatic Parallelization source-to-source tool for C code applications},
   volume = {18},
   url = {https://doi.org/10.1145/3183767.3183770},
   year = {2018},
}
@article{Bae2013,
   abstract = {This paper provides an overview and an evaluation of the Cetus source-to-source compiler infrastructure. The original goal of the Cetus project was to create an easy-to-use compiler for research in automatic parallelization of C programs. In meantime, Cetus has been used for many additional program transformation tasks. It serves as a compiler infrastructure for many projects in the US and internationally. Recently, Cetus has been supported by the National Science Foundation to build a community resource. The compiler has gone through several iterations of benchmark studies and implementations of those techniques that could improve the parallel performance of these programs. These efforts have resulted in a system that favorably compares with state-of-the-art parallelizers, such as Intel's ICC. A key limitation of advanced optimizing compilers is their lack of runtime information, such as the program input data. We will discuss and evaluate several techniques that support dynamic optimization decisions. Finally, as there is an extensive body of proposed compiler analyses and transformations for parallelization, the question of the importance of the techniques arises. This paper evaluates the impact of the individual Cetus techniques on overall program performance.},
   author = {Hansang Bae and Dheya Mustafa and Jae-Woo Lee and Hao Lin and Chirag Dave and Rudolf Eigenmann and Samuel P Midkiff and H Bae and D Mustafa and J-w Lee and H Lin and R Eigenmann and S P Midkiff and C Dave},
   doi = {10.1007/s10766-012-0211-z},
   journal = {Int J Parallel Prog},
   keywords = {Automatic parallelization ·,Compiler infrastructure ·,Performance,Source-to-source translation ·},
   pages = {753-767},
   title = {The Cetus Source-to-Source Compiler Infrastructure: Overview and Evaluation},
   volume = {41},
   year = {2013},
}
@article{Bispo2020,
   abstract = {This article presents Clava, a Clang-based source-to-source compiler, that accepts scripts written in LARA, a JavaScript-based DSL with special constructs for code queries, analysis and transformations. Clava improves Clang's source-to-source capabilities by providing a more convenient and flexible way to analyze, transform and generate C/C++ code, and provides support for building strategies that capture run-time behavior. We present the Clava framework, its main capabilities, and how it can been used. Furthermore, we show that Clava is sufficiently robust to analyze, instrument and test a set of large C/C++ application codes, such as GCC.},
   author = {João Bispo and João M.P. Cardoso},
   doi = {10.1016/J.SOFTX.2020.100565},
   issn = {2352-7110},
   journal = {SoftwareX},
   keywords = {C/C++,Compilers,LARA,Source-to-source},
   month = {7},
   pages = {100565},
   publisher = {Elsevier},
   title = {Clava: C/C++ source-to-source compilation using LARA},
   volume = {12},
   year = {2020},
}
@article{Karer2016,
   abstract = {Eclipse compiler for Java (ECJ) is open source incremented compiler. We reform features of ECJ, related to optimization technique called as dead code detection and elimination. ECJ identifies the dead code. We are extending this compiler to eliminate the dead code. In this paper we are describing the structure of ECJ. For dead code elimination approach we have used Single Static Assignment (SSA) strategy. After applying definition- use (DU) chain on SSA, we detect dead code and eliminate it. In this paper we are describing two algorithms: 1) To convert Java source code into SSA. 2) To eliminate dead code. Hence by implementing dead code elimination in ECJ, we are utilizing space and execution time of program which leads to improve efficiency of compiler.},
   author = {Hiral H. Karer and Purvi B. Soni},
   doi = {10.1109/ICCICCT.2015.7475289},
   isbn = {9781467398251},
   journal = {2015 International Conference on Control Instrumentation Communication and Computational Technologies, ICCICCT 2015},
   keywords = {Directory structure,Efficiency,Optimization},
   month = {5},
   pages = {275-278},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Dead code elimination technique in eclipse compiler for Java},
   year = {2016},
}
@article{Monniaux2021,
   abstract = {We present an approach for implementing a formally certified loop-invariant code motion optimization by composing an unrolling pass and a formally certified yet efficient global subexpression elimination. This approach is lightweight: each pass comes with a simple and independent proof of correctness. Experiments show the approach significantly narrows the performance gap between the CompCert certified compiler and state-of-the-art optimizing compilers. Our static analysis employs an efficient yet verified hashed set structure, resulting in fast compilation.},
   author = {David Monniaux and Cyril Six},
   doi = {10.1145/3461648.3463850},
   isbn = {9781450384728},
   journal = {Proceedings of the ACM SIGPLAN Conference on Languages, Compilers, and Tools for Embedded Systems (LCTES)},
   keywords = {Coq,common subexpression elimination,invariants,optimization,verified compilers,verified hashed sets},
   month = {6},
   pages = {85-96},
   publisher = {Association for Computing Machinery},
   title = {Simple, light, yet formally verified, global common subexpression elimination and loop-invariant code motion},
   year = {2021},
}
@misc{GCCRearch,
   title = {GCC re-architecture - GCC Wiki},
   howpublished = {Available at: \url{https://gcc.gnu.org/wiki/rearch}},
   note = {Accessed 2022-02-16}
}
@article{Novillo2004,
   author = {Diego Novillo},
   journal = {Red Hat Magazine},
   month = {12},
   title = {From Source to Binary: The Inner Workings of GCC},
   note = {Available at: \url{https://web.archive.org/web/20160410185222/https://www.redhat.com/magazine/002dec04/features/gcc/}},
   year = {2004},
}
@misc{CompCertHome,
   title = {CompCert - Main page},
   howpublished = {Available at: \url{https://compcert.org/}},
   note = {Accessed 2022-02-16}
}
@misc{JuliaIR,
   title = {Julia SSA-form IR · The Julia Language},
   howpublished = {Available at: \url{https://docs.julialang.org/en/v1/devdocs/ssair/}},
   note = {Accessed 2022-02-21},
}
@misc{SwiftSIL,
   title = {Swift Intermediate Language Documentation},
   howpublished = {Available at: \url{https://github.com/apple/swift/blob/main/docs/SIL.rst}},
   note = {Accessed 2022-02-21},
}
@inproceedings{Jordan2013,
   abstract = {Programming standards like OpenMP, OpenCL and MPI are frequently considered programming languages for developing parallel applications for their respective kind of architecture. Nevertheless, compilers treat them like ordinary APIs utilized by an otherwise sequential host language. Their parallel control flow remains hidden within opaque runtime library calls which are embedded within a sequential intermediate representation lacking the concepts of parallelism. Consequently, the tuning and coordination of parallelism is clearly beyond the scope of conventional optimizing compilers and hence left to the programmer or the runtime system. The main objective of the Insieme compiler is to overcome this limitation by utilizing INSPIRE, a unified, parallel, high-level intermediate representation. Instead of mapping parallel constructs and APIs to external routines, their behavior is modeled explicitly using a unified and fixed set of parallel language constructs. Making the parallel control flow accessible to the compiler lays the foundation for the development of reusable, static and dynamic analyses and transformations bridging the gap between a variety of parallel paradigms. Within this paper we describe the structure of INSPIRE and elaborate the considerations which influenced its design. Furthermore, we demonstrate its expressiveness by illustrating the encoding of a variety of parallel language constructs and we evaluate its ability to preserve performance relevant aspects of input codes.},
   author = {Herbert Jordan and Simone Pellegrini and Peter Thoman and Klaus Kofler and Thomas Fahringer},
   city = {Edinburgh},
   doi = {10.5555/2523721.2523727},
   isbn = {9781479910212},
   journal = {Proceedings of the 22nd International Conference on Parallel Architectures and Compilation Techniques},
   keywords = {Compiler,High-level Program Analysis,Intermediate Representation,Parallel Computation},
   pages = {7-18},
   publisher = {IEEE Press},
   title = {INSPIRE: The Insieme Parallel Intermediate Representation},
   year = {2013},
}
@phdthesis{Jordan2014,
   abstract = {Developing programs efficiently utilizing contemporary parallel
architectures is complex and time consuming. This is partially
due to the inherent problem of revealing parallelism within algorithms and partially due to the fact that programming languages
and associated compilation tool have not (yet) been adapted to
the fundamental paradigm shift towards parallel systems triggered more than a decade ago.
Available APIs and language extensions for programming
parallel architectures are treated by compilers like ordinary libraries utilized by an otherwise sequential host language. Their
parallel control flow remains hidden within opaque runtime library calls embedded within a sequential intermediate representation lacking the concepts of parallelism. Consequently, the tuning and coordination of parallelism is clearly beyond the scope
of conventional optimizing compilers and hence left to the programmer or the runtime system.
The main objective of the Insieme infrastructure is to provide a platform for researching techniques simplifying the task
of developing efficient, scalable and portable parallel programs
by gradually off-loading the tuning and coordination efforts to
the compiler and the associated runtime system. It is based on a
novel, concise, unified, explicitly parallel, high-level intermediate
representation making the parallel control flow accessible to the
compiler and the associated runtime system. Thus it lays the
foundation for the development of reusable, sophisticated, static
and dynamic utilities handling the workload and data management, the utilization of heterogeneous hardware and tuning steps
for the developer – who, in the end, may only have to focus on
revealing a maximum of parallelism.
Within this thesis, the novel design of the Insieme infrastructure is covered. A particular focus is laid on its internal intermediate representation. Additional chapters elaborate analysis
and transformation techniques built on top of it. Furthermore,
a set of example applications based on the Insieme infrastructure simplifying the development of scalable parallel programs
are outlined.},
   author = {Herbert Jordan},
   city = {Innsbruck},
   institution = {University of Innsbruck},
   month = {8},
   title = {Insieme: A Compiler Infrastructure for Parallel Programs},
   url = {https://diglib.uibk.ac.at/ulbtirolhs/download/pdf/179200?originalFilename=true},
   year = {2014},
}
@inproceedings{Jones1999,
 author = {Simon Peyton Jones and Norman Ramsey and Fermin Reig},
 title = {C--: a portable assembly language that supports garbage collection},
 booktitle = {International Conference on Principles and Practice of Declarative Programming},
 year = {1999},
 month = {January},
 abstract = {For a compiler writer, generating good machine code for a variety of platforms is hard work. One might try to reuse a retargetable code generator, but code generators are complex and difficult to use, and they limit one's choice of implementation language. One might try to use C as a portable assembly language, but C limits the compiler writer's flexibility and the performance of the resulting code. The wide use of C, despite these drawbacks, argues for a portable assembly language. C-- is a new language designed expressly for this purpose. The use of a portable assembly language introduces new problems in the support of such high-level run-time services as garbage collection, exception handling, concurrency, profiling, and debugging. We address these problems by combining the C-- language with a C-- run-time interface. The combination is designed to allow the compiler writer a choice of source-language semantics and implementation techniques, while still providing good performance.},
 url = {https://www.microsoft.com/en-us/research/publication/portable-assembly-language-supports-garbage-collection/},
 pages = {1-28},
 edition = {International Conference on Principles and Practice of Declarative Programming},
 }


